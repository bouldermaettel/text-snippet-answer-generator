# Snippet Answer – RAG Q&A MVP

Answer questions from a growing collection of text snippets. Uses semantic search + optional LLM (Azure OpenAI or Ollama) with source attribution and confidence scores.

## Quick start

### 1. Backend

```bash
cd backend
python -m venv .venv
source .venv/bin/activate   # or .venv\Scripts\activate on Windows
pip install -r requirements.txt
cp .env.example .env        # optional: set AZURE_OPENAI_* or use Ollama
uvicorn app.main:app --reload --port 8000
```

- **LLM**: Set `AZURE_OPENAI_ENDPOINT` and `AZURE_OPENAI_API_KEY` for Azure, or run [Ollama](https://ollama.ai) locally (`ollama serve`, `ollama pull llama3.2`) for local answers. If neither is set, the app falls back to returning the top snippet as the answer.

### 2. Frontend

```bash
cd frontend
npm install
npm run dev
```

Open http://localhost:5173. Use the **sidebar** to switch between **Ask** and **Collection**. Add snippets (or upload .txt, .docx, or .pdf), assign them to **groups**, then in the Ask tab choose to search **all snippets**, **selected groups**, or **selected snippets**.

### 3. Seed snippets (optional)

With the backend running:

```bash
cd backend
curl -X POST http://localhost:8000/api/snippets -H "Content-Type: application/json" -d '{"text":"Our refund window is 30 days from purchase. Contact support with your order ID.","title":"Refund policy"}'
curl -X POST http://localhost:8000/api/snippets -H "Content-Type: application/json" -d '{"text":"Shipping usually takes 3–5 business days within the EU.","title":"Shipping"}'
```

Then ask e.g. “What is your refund policy?” in the UI.

## Features

- **Sidebar**: Switch between Ask and Collection; filter Collection by group.
- **Groups**: Assign snippets to groups when adding/editing; filter the Ask search by “all”, “selected groups”, or “selected snippets”.
- **Chunking**: Snippets longer than `CHUNK_SIZE` (default 1500 chars) are split into overlapping chunks; if any chunk matches a question, all chunks of that snippet are retrieved for the answer.
- **PDF upload**: Upload .txt, .docx, or .pdf; each file becomes one snippet (chunked if large). If `UPLOAD_DIR` is set in the backend env, uploaded PDF/DOCX are stored and a "View original document" link is shown in the collection and in answer sources.

## How it works

The application implements a **Retrieval-Augmented Generation (RAG)** pipeline with several advanced retrieval techniques to improve answer quality. Below is a walkthrough of the key components.

### Embedding & vector store

Snippets are embedded using **sentence-transformers** (`all-MiniLM-L6-v2` by default) or, optionally, **Azure OpenAI embeddings** (`text-embedding-3-small`). Embeddings are stored in a local **ChromaDB** persistent vector database. When a snippet exceeds `CHUNK_SIZE` (default 1500 chars), it is split into overlapping chunks (`CHUNK_OVERLAP`, default 200 chars). At query time, if any chunk matches, all chunks of the parent snippet are reassembled so the LLM sees the full context.

### Retrieval pipeline

The retrieval step runs up to **three parallel scoring paths** before selecting the final top-k results:

#### 1. HyDE – Hypothetical Document Embeddings

When enabled (`use_hyde=true` on the ask request), the system first asks the LLM to generate a **short hypothetical answer** (1-2 sentences) to the user's question — without consulting any snippets. This hypothetical answer is then embedded and used as the query vector instead of the raw question. Because the hypothetical answer is linguistically closer to the stored snippet text than a question would be, it often produces better semantic matches, especially for domain-specific or factual content.

*Fallback*: if no LLM is configured, HyDE is silently skipped and the raw question embedding is used.

#### 2. Example question search (hybrid retrieval)

Each snippet can have one or more **example questions** associated with it (added manually or auto-generated by the LLM via "reverse HyDE"). These example questions are embedded and stored in a **separate ChromaDB collection**.

At query time the user's raw question (not the HyDE hypothetical) is embedded and searched against this example-question collection in parallel with the snippet-text search. Results from both paths are **merged by snippet ID** using a weighted confidence score:

```
combined = (1 - eq_weight) × snippet_confidence + eq_weight × example_question_confidence
```

The default `eq_weight` is **0.3**. Snippets found via both paths receive a score boost; snippets found in only one path keep their single-source score.

#### 3. Keyword reranking

After the vector search(es) return a candidate list (2× `top_k`), a **keyword overlap score** is computed for each candidate:

```
keyword_score = (# of question tokens found in snippet) / (# of question tokens)
```

The final ranking uses a linear blend:

```
reranked_score = 0.7 × semantic_confidence + 0.3 × keyword_score
```

This helps surface snippets that share exact terminology with the question, which pure semantic search can sometimes miss.

### Confidence scoring

- **Snippet confidence**: ChromaDB returns L2 distances for normalised embeddings (range roughly 0–2). These are mapped to a 0–1 confidence via `1 − distance / 2`.
- **Answer confidence**: a weighted aggregate of the top-3 snippet confidences: `0.6 × max(top3) + 0.4 × mean(top3)`.

### Answer generation

The top-k snippets are passed to the LLM (Azure OpenAI or Ollama) as numbered context. The system prompt instructs the model to answer the question using the snippets, respecting an **answer closeness** slider (0 = free rephrasing, 1 = stick to snippet wording). The LLM also returns a **section label** per source snippet (e.g. "Scrum Roles – Product Owner") which is displayed alongside each source card in the UI.

If no LLM is configured, the system falls back to returning the top snippet verbatim.

### Answer refinement

After an initial answer is generated, users can submit a **refinement prompt** (e.g. "make it shorter", "focus on the deadline policy"). The system sends the original question, the original answer, the user's feedback, and the selected source snippets back to the LLM to produce an improved version.

### Cross-language translation indexing

When translation indexing is enabled, each snippet is automatically:

1. **Language-detected** — via the `langdetect` library (fast, no API call), with LLM fallback.
2. **Translated** — the LLM translates the snippet into every configured target language (`TRANSLATION_LANGUAGES`, default `en,de,fr,it`) that is not already covered.
3. **Indexed** — each translation is stored as additional chunks under the same parent snippet ID, tagged with `is_translation=true` and the target language. This means a German question can match an English snippet's German translation, enabling **cross-language semantic retrieval** without a multilingual embedding model.

Snippets can also declare **linked snippets** (manually curated translations) via metadata; the system detects which languages are already covered and only generates LLM translations for the missing ones.

### Architecture diagram

```
User question
     │
     ├──[HyDE]──▶ LLM generates hypothetical answer ──▶ embed
     │                                                      │
     │              ┌───────────────────────────────────────┘
     │              ▼
     │     ChromaDB: snippet text collection ──────────┐
     │                                                  │  merge by
     ├──[raw embed]──▶ ChromaDB: example questions ────┤  snippet ID
     │                                                  │
     │              ┌───────────────────────────────────┘
     │              ▼
     │     Keyword reranking (question token overlap)
     │              │
     │              ▼
     │     Top-k snippets + confidence scores
     │              │
     │              ▼
     └────▶ LLM answer generation (with closeness control)
                    │
                    ▼
           Answer + sources + section labels + confidence
```

## Project layout

- `backend/app/` – FastAPI app, ChromaDB store, embeddings, retrieval, chunking, Azure/Ollama generation
- `frontend/` – React + Vite + TypeScript + Tailwind; sidebar, ask with scope, collection by group, add/edit snippet with group

## Troubleshooting

- **"Failed to fetch" or "Cannot reach the backend"**: Ensure the backend is running on port 8000 (`cd backend && uvicorn app.main:app --reload --port 8000`) and you're using the frontend from the Vite dev server (`cd frontend && npm run dev`), then open http://localhost:5173.
- **Document link (PDF/DOCX) doesn't open**: Create `frontend/.env` with `VITE_API_BASE_URL=http://localhost:8000` so "View in original document" opens the file from the backend directly.
- **SSL / certificate errors** when downloading the embedding model from Hugging Face (e.g. behind a proxy or corporate network): set `SSL_CERT_FILE` or `REQUESTS_CA_BUNDLE` to your CA bundle, or pre-download the model once with a working connection and reuse the cache. The embedding model is loaded on first use (first ask or add-snippet), not at server startup.

## Environment (backend)

See `backend/.env.example`. Main options:

- **Azure OpenAI (LLM)**: `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_CHAT_DEPLOYMENT`
- **Azure OpenAI (embeddings)**: set `AZURE_OPENAI_EMBEDDING_DEPLOYMENT` (e.g. `text-embedding-3-small`) along with the same endpoint and API key to use Azure for snippet embeddings instead of the local sentence-transformers model.
- **Ollama**: `OLLAMA_BASE_URL` (default `http://localhost:11434`), `OLLAMA_CHAT_MODEL` (e.g. `llama3.2`)
- `LLM_PROVIDER`: `auto` (default), `azure`, `ollama`, or `none`
- **Chunking**: `CHUNK_SIZE` (default `1500` chars), `CHUNK_OVERLAP` (default `200`) for splitting large snippets
- **Uploads**: `UPLOAD_DIR` (default `data/uploads`; set empty to disable) – uploaded PDF/DOCX are saved and linked from snippets so users can open the original document. For document links to open in a new tab, set `VITE_API_BASE_URL=http://localhost:8000` in `frontend/.env`.
